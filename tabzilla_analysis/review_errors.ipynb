{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review errors in the results files `metadataset_errors*.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "errors_df = pd.read_csv(\"/home/shared/tabzilla/TabSurvey/metadataset_errors_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>results_bucket_path</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>alg_name</th>\n",
       "      <th>hparam_source</th>\n",
       "      <th>trial_number</th>\n",
       "      <th>alg_hparam_id</th>\n",
       "      <th>exp_name</th>\n",
       "      <th>exception</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>results/openml__APSFailure__168868/KNN/cpu-exp...</td>\n",
       "      <td>openml__APSFailure__168868</td>\n",
       "      <td>KNN</td>\n",
       "      <td>random_1_s0</td>\n",
       "      <td>1</td>\n",
       "      <td>KNN__seed_0__trial_1</td>\n",
       "      <td>cpu-expt_091822_200045_4107.zip</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>results/openml__APSFailure__168868/KNN/cpu-exp...</td>\n",
       "      <td>openml__APSFailure__168868</td>\n",
       "      <td>KNN</td>\n",
       "      <td>random_10_s0</td>\n",
       "      <td>10</td>\n",
       "      <td>KNN__seed_0__trial_10</td>\n",
       "      <td>cpu-expt_091822_200045_4107.zip</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>results/openml__APSFailure__168868/KNN/cpu-exp...</td>\n",
       "      <td>openml__APSFailure__168868</td>\n",
       "      <td>KNN</td>\n",
       "      <td>random_11_s0</td>\n",
       "      <td>11</td>\n",
       "      <td>KNN__seed_0__trial_11</td>\n",
       "      <td>cpu-expt_091822_200045_4107.zip</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>results/openml__APSFailure__168868/KNN/cpu-exp...</td>\n",
       "      <td>openml__APSFailure__168868</td>\n",
       "      <td>KNN</td>\n",
       "      <td>random_12_s0</td>\n",
       "      <td>12</td>\n",
       "      <td>KNN__seed_0__trial_12</td>\n",
       "      <td>cpu-expt_091822_200045_4107.zip</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>results/openml__APSFailure__168868/KNN/cpu-exp...</td>\n",
       "      <td>openml__APSFailure__168868</td>\n",
       "      <td>KNN</td>\n",
       "      <td>random_2_s0</td>\n",
       "      <td>2</td>\n",
       "      <td>KNN__seed_0__trial_2</td>\n",
       "      <td>cpu-expt_091822_200045_4107.zip</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 results_bucket_path  \\\n",
       "0  results/openml__APSFailure__168868/KNN/cpu-exp...   \n",
       "1  results/openml__APSFailure__168868/KNN/cpu-exp...   \n",
       "2  results/openml__APSFailure__168868/KNN/cpu-exp...   \n",
       "3  results/openml__APSFailure__168868/KNN/cpu-exp...   \n",
       "4  results/openml__APSFailure__168868/KNN/cpu-exp...   \n",
       "\n",
       "                 dataset_name alg_name hparam_source  trial_number  \\\n",
       "0  openml__APSFailure__168868      KNN   random_1_s0             1   \n",
       "1  openml__APSFailure__168868      KNN  random_10_s0            10   \n",
       "2  openml__APSFailure__168868      KNN  random_11_s0            11   \n",
       "3  openml__APSFailure__168868      KNN  random_12_s0            12   \n",
       "4  openml__APSFailure__168868      KNN   random_2_s0             2   \n",
       "\n",
       "           alg_hparam_id                         exp_name  \\\n",
       "0   KNN__seed_0__trial_1  cpu-expt_091822_200045_4107.zip   \n",
       "1  KNN__seed_0__trial_10  cpu-expt_091822_200045_4107.zip   \n",
       "2  KNN__seed_0__trial_11  cpu-expt_091822_200045_4107.zip   \n",
       "3  KNN__seed_0__trial_12  cpu-expt_091822_200045_4107.zip   \n",
       "4   KNN__seed_0__trial_2  cpu-expt_091822_200045_4107.zip   \n",
       "\n",
       "                                           exception  \n",
       "0  Traceback (most recent call last):\\n  File \"/h...  \n",
       "1  Traceback (most recent call last):\\n  File \"/h...  \n",
       "2  Traceback (most recent call last):\\n  File \"/h...  \n",
       "3  Traceback (most recent call last):\\n  File \"/h...  \n",
       "4  Traceback (most recent call last):\\n  File \"/h...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors by dataset:\n",
      " openml__CIFAR_10__167124                                  73\n",
      "openml__sylvine__168912                                   60\n",
      "openml__guillermo__168337                                 51\n",
      "openml__riccardo__168338                                  51\n",
      "openml__dionis__189355                                    45\n",
      "openml__sulfur__360966                                    30\n",
      "openml__mnist_784__3573                                   26\n",
      "openml__robert__168332                                    24\n",
      "openml__Fashion-MNIST__146825                             20\n",
      "openml__albert__189356                                    19\n",
      "openml__covertype__7593                                   18\n",
      "openml__Devnagari-Script__167121                          18\n",
      "openml__skin-segmentation__9965                           17\n",
      "openml__APSFailure__168868                                17\n",
      "openml__higgs__146606                                     16\n",
      "openml__jannis__168330                                    13\n",
      "openml__connect-4__146195                                 12\n",
      "openml__numerai28.6__167120                               11\n",
      "openml__dilbert__168909                                   10\n",
      "openml__electricity__219                                  10\n",
      "openml__airlines__189354                                  10\n",
      "openml__MiniBooNE__168335                                  8\n",
      "openml__spambase__43                                       7\n",
      "openml__first-order-theorem-proving__9985                  7\n",
      "openml__steel-plates-fault__146817                         7\n",
      "openml__volkert__168331                                    7\n",
      "openml__credit-g__31                                       7\n",
      "openml__nomao__9977                                        7\n",
      "openml__diabetes__37                                       6\n",
      "openml__jungle_chess_2pcs_raw_endgame_complete__167119     6\n",
      "openml__helena__168329                                     6\n",
      "openml__kc1__3917                                          6\n",
      "openml__california__361089                                 5\n",
      "openml__adult-census__3953                                 5\n",
      "openml__cmc__23                                            5\n",
      "openml__adult__7592                                        4\n",
      "openml__sick__3021                                         4\n",
      "openml__bank-marketing__14965                              4\n",
      "openml__churn__167141                                      4\n",
      "openml__isolet__3481                                       3\n",
      "openml__Bioresponse__9910                                  3\n",
      "openml__shuttle__146212                                    3\n",
      "openml__mfeat-morphological__18                            3\n",
      "openml__mv__4774                                           3\n",
      "openml__har__14970                                         3\n",
      "openml__GesturePhaseSegmentationProcessed__14969           2\n",
      "openml__blood-transfusion-service-center__145836           2\n",
      "openml__phoneme__9952                                      2\n",
      "openml__jm1__3904                                          2\n",
      "openml__ilpd__9971                                         2\n",
      "openml__haberman__42                                       2\n",
      "openml__fabert__168910                                     2\n",
      "openml__blood-transfusion-service-center__10101            2\n",
      "openml__wilt__146820                                       2\n",
      "openml__letter__6                                          1\n",
      "openml__Internet-Advertisements__167125                    1\n",
      "openml__christine__168908                                  1\n",
      "Name: dataset_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# how many errors by dataset?\n",
    "print(f\"errors by dataset:\\n {errors_df['dataset_name'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors by alg:\n",
      " SVM         174\n",
      "VIME        154\n",
      "TabNet      111\n",
      "KNN          99\n",
      "LightGBM     91\n",
      "MLP          60\n",
      "XGBoost       6\n",
      "Name: alg_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# by alg...\n",
    "print(f\"errors by alg:\\n {errors_df['alg_name'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triage errors by alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_by_alg = {}\n",
    "for alg_name in errors_df[\"alg_name\"].unique():\n",
    "    errors_by_alg[alg_name] = errors_df.loc[errors_df[\"alg_name\"] == alg_name, \"exception\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors for alg SVM: 174\n",
      "number of unique errors: 9\n"
     ]
    }
   ],
   "source": [
    "alg_name = \"SVM\"\n",
    "\n",
    "print(f\"number of errors for alg {alg_name}: {len(errors_by_alg[alg_name])}\")\n",
    "print(f\"number of unique errors: {len(set(errors_by_alg[alg_name]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the unique errors?\n",
    "unique_errors = list(set(errors_by_alg[alg_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 4\n",
      "\n",
      "\n",
      "error 1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 3\n",
      "\n",
      "\n",
      "error 2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 1\n",
      "\n",
      "\n",
      "error 3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 7\n",
      "\n",
      "\n",
      "error 4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 2\n",
      "\n",
      "\n",
      "error 5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 9\n",
      "\n",
      "\n",
      "error 6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 8\n",
      "\n",
      "\n",
      "error 7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 6\n",
      "\n",
      "\n",
      "error 8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(unique_errors):\n",
    "    print(f\"error {i}:\\n{e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All caught SVM errors are time limits issues, for a single hyperparameter sample. Let's see if this is due to the same hparam sample... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_timeout_errors = errors_df.loc[(errors_df[\"alg_name\"] == alg_name) & (~errors_df[\"exception\"].str.contains(\"Timeout\")) , \"exception\"].unique() #   \"dataset_name\", \"hparam_source\",\n",
    "\n",
    "for i, e in enumerate(non_timeout_errors):\n",
    "    print(f\"----exception {i}----\")\n",
    "    print(f\"occurrences: {len(errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)])}\")\n",
    "    print(e + \"\\n\")\n",
    "\n",
    "    er_datasets = errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)][\"dataset_name\"].unique()\n",
    "    print(\"this exception occurs on the following datasets:\")\n",
    "    for d in er_datasets:\n",
    "        print(d)\n",
    "\n",
    "    print(f\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors for alg VIME: 154\n",
      "number of unique errors: 12\n"
     ]
    }
   ],
   "source": [
    "alg_name = \"VIME\"\n",
    "\n",
    "print(f\"number of errors for alg {alg_name}: {len(errors_by_alg[alg_name])}\")\n",
    "print(f\"number of unique errors: {len(set(errors_by_alg[alg_name]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the unique errors?\n",
    "unique_errors = list(set(errors_by_alg[alg_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 4\n",
      "\n",
      "\n",
      "error 1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 47, in fit\n",
      "    self.fit_self(X_unlab, p_m=self.params[\"p_m\"], alpha=self.params[\"alpha\"])\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 148, in fit_self\n",
      "    for batch_X, batch_mask, batch_feat in train_loader:\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 368, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 314, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 927, in __init__\n",
      "    w.start()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/context.py\", line 224, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 66, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "\n",
      "error 2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 3\n",
      "\n",
      "\n",
      "error 3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 1\n",
      "\n",
      "\n",
      "error 4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 54, in fit\n",
      "    loss_history, val_loss_history = self.fit_semi(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 248, in fit_semi\n",
      "    val_loss += loss_func_supervised(y_hat, batch_val_y.to(self.device))\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 713, in forward\n",
      "    return F.binary_cross_entropy_with_logits(input, target,\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py\", line 3130, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
      "ValueError: Target size (torch.Size([1])) must be the same as input size (torch.Size([]))\n",
      "\n",
      "\n",
      "error 5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 7\n",
      "\n",
      "\n",
      "error 6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 2\n",
      "\n",
      "\n",
      "error 7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 9\n",
      "\n",
      "\n",
      "error 8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 8\n",
      "\n",
      "\n",
      "error 9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 47, in fit\n",
      "    self.fit_self(X_unlab, p_m=self.params[\"p_m\"], alpha=self.params[\"alpha\"])\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 136, in fit_self\n",
      "    x_tilde = torch.tensor(x_tilde).float()\n",
      "RuntimeError: [enforce fail at alloc_cpu.cpp:73] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 663552000 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n",
      "\n",
      "error 10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 6\n",
      "\n",
      "\n",
      "error 11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(unique_errors):\n",
    "    print(f\"error {i}:\\n{e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three VIME errors are due to memory, not timeout. let's look at these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----exception 0----\n",
      "occurrences: 30\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 47, in fit\n",
      "    self.fit_self(X_unlab, p_m=self.params[\"p_m\"], alpha=self.params[\"alpha\"])\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 136, in fit_self\n",
      "    x_tilde = torch.tensor(x_tilde).float()\n",
      "RuntimeError: [enforce fail at alloc_cpu.cpp:73] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 663552000 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__CIFAR_10__167124\n",
      "--------\n",
      "----exception 1----\n",
      "occurrences: 13\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 47, in fit\n",
      "    self.fit_self(X_unlab, p_m=self.params[\"p_m\"], alpha=self.params[\"alpha\"])\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 148, in fit_self\n",
      "    for batch_X, batch_mask, batch_feat in train_loader:\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 368, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 314, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 927, in __init__\n",
      "    w.start()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/context.py\", line 224, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 66, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__dionis__189355\n",
      "--------\n",
      "----exception 2----\n",
      "occurrences: 30\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 54, in fit\n",
      "    loss_history, val_loss_history = self.fit_semi(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/vime.py\", line 248, in fit_semi\n",
      "    val_loss += loss_func_supervised(y_hat, batch_val_y.to(self.device))\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 713, in forward\n",
      "    return F.binary_cross_entropy_with_logits(input, target,\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py\", line 3130, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
      "ValueError: Target size (torch.Size([1])) must be the same as input size (torch.Size([]))\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__sylvine__168912\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "non_timeout_errors = errors_df.loc[(errors_df[\"alg_name\"] == alg_name) & (~errors_df[\"exception\"].str.contains(\"Timeout\")) , \"exception\"].unique() #   \"dataset_name\", \"hparam_source\",\n",
    "\n",
    "for i, e in enumerate(non_timeout_errors):\n",
    "    print(f\"----exception {i}----\")\n",
    "    print(f\"occurrences: {len(errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)])}\")\n",
    "    print(e + \"\\n\")\n",
    "\n",
    "    er_datasets = errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)][\"dataset_name\"].unique()\n",
    "    print(\"this exception occurs on the following datasets:\")\n",
    "    for d in er_datasets:\n",
    "        print(d)\n",
    "\n",
    "    print(f\"--------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some of the hparam sets seem pathological... like `random_2_s0`. But many other hparam sets yield errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors for alg TabNet: 111\n",
      "number of unique errors: 10\n"
     ]
    }
   ],
   "source": [
    "alg_name = \"TabNet\"\n",
    "\n",
    "print(f\"number of errors for alg {alg_name}: {len(errors_by_alg[alg_name])}\")\n",
    "print(f\"number of unique errors: {len(set(errors_by_alg[alg_name]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the unique errors?\n",
    "unique_errors = list(set(errors_by_alg[alg_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 4\n",
      "\n",
      "\n",
      "error 1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/tabnet.py\", line 45, in fit\n",
      "    self.model.fit(\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py\", line 223, in fit\n",
      "    self._train_epoch(train_dataloader)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py\", line 434, in _train_epoch\n",
      "    batch_logs = self._train_batch(X, y)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py\", line 469, in _train_batch\n",
      "    output, M_loss = self.network(X)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py\", line 583, in forward\n",
      "    return self.tabnet(x)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py\", line 468, in forward\n",
      "    steps_output, M_loss = self.encoder(x)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py\", line 150, in forward\n",
      "    x = self.initial_bn(x)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py\", line 2419, in batch_norm\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py\", line 2387, in _verify_batch_size\n",
      "    raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 6])\n",
      "\n",
      "\n",
      "error 2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 3\n",
      "\n",
      "\n",
      "error 3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 1\n",
      "\n",
      "\n",
      "error 4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 7\n",
      "\n",
      "\n",
      "error 5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 2\n",
      "\n",
      "\n",
      "error 6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 9\n",
      "\n",
      "\n",
      "error 7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 8\n",
      "\n",
      "\n",
      "error 8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 6\n",
      "\n",
      "\n",
      "error 9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(unique_errors):\n",
    "    print(f\"error {i}:\\n{e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a couple of TabNet errors are not timeout. Let's look at those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----exception 0----\n",
      "occurrences: 30\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/tabnet.py\", line 45, in fit\n",
      "    self.model.fit(\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py\", line 223, in fit\n",
      "    self._train_epoch(train_dataloader)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py\", line 434, in _train_epoch\n",
      "    batch_logs = self._train_batch(X, y)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py\", line 469, in _train_batch\n",
      "    output, M_loss = self.network(X)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py\", line 583, in forward\n",
      "    return self.tabnet(x)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py\", line 468, in forward\n",
      "    steps_output, M_loss = self.encoder(x)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py\", line 150, in forward\n",
      "    x = self.initial_bn(x)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 168, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py\", line 2419, in batch_norm\n",
      "    _verify_batch_size(input.size())\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py\", line 2387, in _verify_batch_size\n",
      "    raise ValueError(\"Expected more than 1 value per channel when training, got input size {}\".format(size))\n",
      "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 6])\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__sulfur__360966\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "non_timeout_errors = errors_df.loc[(errors_df[\"alg_name\"] == alg_name) & (~errors_df[\"exception\"].str.contains(\"Timeout\")) , \"exception\"].unique() #   \"dataset_name\", \"hparam_source\",\n",
    "\n",
    "for i, e in enumerate(non_timeout_errors):\n",
    "    print(f\"----exception {i}----\")\n",
    "    print(f\"occurrences: {len(errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)])}\")\n",
    "    print(e + \"\\n\")\n",
    "\n",
    "    er_datasets = errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)][\"dataset_name\"].unique()\n",
    "    print(\"this exception occurs on the following datasets:\")\n",
    "    for d in er_datasets:\n",
    "        print(d)\n",
    "\n",
    "    print(f\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors_df.loc[errors_df[\"alg_name\"] == alg_name, \"hparam_source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors for alg KNN: 99\n",
      "number of unique errors: 9\n"
     ]
    }
   ],
   "source": [
    "alg_name = \"KNN\"\n",
    "\n",
    "print(f\"number of errors for alg {alg_name}: {len(errors_by_alg[alg_name])}\")\n",
    "print(f\"number of unique errors: {len(set(errors_by_alg[alg_name]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the unique errors?\n",
    "unique_errors = list(set(errors_by_alg[alg_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 4\n",
      "\n",
      "\n",
      "error 1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 3\n",
      "\n",
      "\n",
      "error 2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 1\n",
      "\n",
      "\n",
      "error 3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 7\n",
      "\n",
      "\n",
      "error 4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 2\n",
      "\n",
      "\n",
      "error 5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 9\n",
      "\n",
      "\n",
      "error 6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 8\n",
      "\n",
      "\n",
      "error 7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 6\n",
      "\n",
      "\n",
      "error 8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(unique_errors):\n",
    "    print(f\"error {i}:\\n{e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All caught KNN errors are time limits issues, for a single hyperparameter sample. Let's see if this is due to the same hparam sample... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors_df.loc[errors_df[\"alg_name\"] == \"KNN\", \"hparam_source\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_timeout_errors = errors_df.loc[(errors_df[\"alg_name\"] == alg_name) & (~errors_df[\"exception\"].str.contains(\"Timeout\")) , \"exception\"].unique() #   \"dataset_name\", \"hparam_source\",\n",
    "\n",
    "for i, e in enumerate(non_timeout_errors):\n",
    "    print(f\"----exception {i}----\")\n",
    "    print(f\"occurrences: {len(errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)])}\")\n",
    "    print(e + \"\\n\")\n",
    "\n",
    "    er_datasets = errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)][\"dataset_name\"].unique()\n",
    "    print(\"this exception occurs on the following datasets:\")\n",
    "    for d in er_datasets:\n",
    "        print(d)\n",
    "\n",
    "    print(f\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors for alg MLP: 60\n",
      "number of unique errors: 2\n"
     ]
    }
   ],
   "source": [
    "alg_name = \"MLP\"\n",
    "\n",
    "print(f\"number of errors for alg {alg_name}: {len(errors_by_alg[alg_name])}\")\n",
    "print(f\"number of unique errors: {len(set(errors_by_alg[alg_name]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the unique errors?\n",
    "unique_errors = list(set(errors_by_alg[alg_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/mlp.py\", line 31, in fit\n",
      "    return super().fit(X, y, X_val, y_val)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/basemodel_torch.py\", line 121, in fit\n",
      "    val_loss += loss_func(out, batch_val_y.to(self.device))\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 713, in forward\n",
      "    return F.binary_cross_entropy_with_logits(input, target,\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py\", line 3130, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
      "ValueError: Target size (torch.Size([1])) must be the same as input size (torch.Size([]))\n",
      "\n",
      "\n",
      "error 1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/mlp.py\", line 31, in fit\n",
      "    return super().fit(X, y, X_val, y_val)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/basemodel_torch.py\", line 92, in fit\n",
      "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 368, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 314, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 927, in __init__\n",
      "    w.start()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/context.py\", line 224, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 66, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(unique_errors):\n",
    "    print(f\"error {i}:\\n{e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP errors are not timeout errors... let's look more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----exception 0----\n",
      "occurrences: 30\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/mlp.py\", line 31, in fit\n",
      "    return super().fit(X, y, X_val, y_val)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/basemodel_torch.py\", line 92, in fit\n",
      "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 368, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 314, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 927, in __init__\n",
      "    w.start()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/context.py\", line 224, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 66, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__dionis__189355\n",
      "--------\n",
      "----exception 1----\n",
      "occurrences: 30\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/mlp.py\", line 31, in fit\n",
      "    return super().fit(X, y, X_val, y_val)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/basemodel_torch.py\", line 121, in fit\n",
      "    val_loss += loss_func(out, batch_val_y.to(self.device))\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 713, in forward\n",
      "    return F.binary_cross_entropy_with_logits(input, target,\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py\", line 3130, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
      "ValueError: Target size (torch.Size([1])) must be the same as input size (torch.Size([]))\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__sylvine__168912\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "non_timeout_errors = errors_df.loc[(errors_df[\"alg_name\"] == alg_name) & (~errors_df[\"exception\"].str.contains(\"Timeout\")) , \"exception\"].unique() #   \"dataset_name\", \"hparam_source\",\n",
    "\n",
    "for i, e in enumerate(non_timeout_errors):\n",
    "    print(f\"----exception {i}----\")\n",
    "    print(f\"occurrences: {len(errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)])}\")\n",
    "    print(e + \"\\n\")\n",
    "\n",
    "    er_datasets = errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)][\"dataset_name\"].unique()\n",
    "    print(\"this exception occurs on the following datasets:\")\n",
    "    for d in er_datasets:\n",
    "        print(d)\n",
    "\n",
    "    print(f\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors for alg XGBoost: 6\n",
      "number of unique errors: 3\n"
     ]
    }
   ],
   "source": [
    "alg_name = \"XGBoost\"\n",
    "\n",
    "print(f\"number of errors for alg {alg_name}: {len(errors_by_alg[alg_name])}\")\n",
    "print(f\"number of unique errors: {len(set(errors_by_alg[alg_name]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the unique errors?\n",
    "unique_errors = list(set(errors_by_alg[alg_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 8\n",
      "\n",
      "\n",
      "error 1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 6\n",
      "\n",
      "\n",
      "error 2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(unique_errors):\n",
    "    print(f\"error {i}:\\n{e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All caught XGB errors are time limits issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_timeout_errors = errors_df.loc[(errors_df[\"alg_name\"] == alg_name) & (~errors_df[\"exception\"].str.contains(\"Timeout\")) , \"exception\"].unique() #   \"dataset_name\", \"hparam_source\",\n",
    "\n",
    "for i, e in enumerate(non_timeout_errors):\n",
    "    print(f\"----exception {i}----\")\n",
    "    print(f\"occurrences: {len(errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)])}\")\n",
    "    print(e + \"\\n\")\n",
    "\n",
    "    er_datasets = errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)][\"dataset_name\"].unique()\n",
    "    print(\"this exception occurs on the following datasets:\")\n",
    "    for d in er_datasets:\n",
    "        print(d)\n",
    "\n",
    "    print(f\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----exception 0----\n",
      "occurrences: 30\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/mlp.py\", line 31, in fit\n",
      "    return super().fit(X, y, X_val, y_val)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/basemodel_torch.py\", line 92, in fit\n",
      "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 368, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 314, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 927, in __init__\n",
      "    w.start()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/context.py\", line 224, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/multiprocessing/popen_fork.py\", line 66, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__dionis__189355\n",
      "--------\n",
      "----exception 1----\n",
      "occurrences: 30\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 237, in cross_validation\n",
      "    loss_history, val_loss_history = curr_model.fit(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/mlp.py\", line 31, in fit\n",
      "    return super().fit(X, y, X_val, y_val)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/basemodel_torch.py\", line 121, in fit\n",
      "    val_loss += loss_func(out, batch_val_y.to(self.device))\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 713, in forward\n",
      "    return F.binary_cross_entropy_with_logits(input, target,\n",
      "  File \"/opt/conda/envs/torch/lib/python3.10/site-packages/torch/nn/functional.py\", line 3130, in binary_cross_entropy_with_logits\n",
      "    raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
      "ValueError: Target size (torch.Size([1])) must be the same as input size (torch.Size([]))\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__sylvine__168912\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "non_timeout_errors = errors_df.loc[(errors_df[\"alg_name\"] == alg_name) & (~errors_df[\"exception\"].str.contains(\"Timeout\")) , \"exception\"].unique() #   \"dataset_name\", \"hparam_source\",\n",
    "\n",
    "for i, e in enumerate(non_timeout_errors):\n",
    "    print(f\"----exception {i}----\")\n",
    "    print(f\"occurrences: {len(errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)])}\")\n",
    "    print(e + \"\\n\")\n",
    "\n",
    "    er_datasets = errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)][\"dataset_name\"].unique()\n",
    "    print(\"this exception occurs on the following datasets:\")\n",
    "    for d in er_datasets:\n",
    "        print(d)\n",
    "\n",
    "    print(f\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors for alg LightGBM: 91\n",
      "number of unique errors: 8\n"
     ]
    }
   ],
   "source": [
    "alg_name = \"LightGBM\"\n",
    "\n",
    "print(f\"number of errors for alg {alg_name}: {len(errors_by_alg[alg_name])}\")\n",
    "print(f\"number of unique errors: {len(set(errors_by_alg[alg_name]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the unique errors?\n",
    "unique_errors = list(set(errors_by_alg[alg_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 1\n",
      "\n",
      "\n",
      "error 1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 822, in dispatch_one_batch\n",
      "    tasks = self._ready_batches.get(block=False)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/queue.py\", line 168, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 217, in cross_validation\n",
      "    processed_data = process_data(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_data_processing.py\", line 41, in process_data\n",
      "    X_train = preprocessor.fit_transform(X_train)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 673, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 604, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 833, in dispatch_one_batch\n",
      "    islice = list(itertools.islice(iterator, big_batch_size))\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 607, in <genexpr>\n",
      "    X=_safe_indexing(X, column, axis=1),\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 361, in _safe_indexing\n",
      "    return _array_indexing(X, indices, indices_dtype, axis=axis)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 185, in _array_indexing\n",
      "    return array[key] if axis == 0 else array[:, key]\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 524. MiB for an array with shape (4296, 16000) and data type float64\n",
      "\n",
      "\n",
      "error 2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 217, in cross_validation\n",
      "    processed_data = process_data(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_data_processing.py\", line 41, in process_data\n",
      "    X_train = preprocessor.fit_transform(X_train)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 673, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 604, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 422, in fit_transform\n",
      "    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/base.py\", line 867, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 345, in fit\n",
      "    X = self._validate_input(X, in_fit=True)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 287, in _validate_input\n",
      "    X = self._validate_data(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/base.py\", line 577, in _validate_data\n",
      "    X = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 925, in check_array\n",
      "    array = np.array(array, dtype=dtype, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 524. MiB for an array with shape (16000, 4296) and data type float64\n",
      "\n",
      "\n",
      "error 3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 217, in cross_validation\n",
      "    processed_data = process_data(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_data_processing.py\", line 41, in process_data\n",
      "    X_train = preprocessor.fit_transform(X_train)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 673, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 604, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 422, in fit_transform\n",
      "    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/base.py\", line 867, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 384, in fit\n",
      "    self.statistics_ = self._dense_fit(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 436, in _dense_fit\n",
      "    mean_masked = np.ma.mean(masked_X, axis=0)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/numpy/ma/core.py\", line 6774, in __call__\n",
      "    return method(marr, *args, **params)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/numpy/ma/core.py\", line 5244, in mean\n",
      "    cnt = self.count(axis=axis, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/numpy/ma/core.py\", line 4534, in count\n",
      "    return (~m).sum(axis=axis, dtype=np.intp, **kwargs)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 141. MiB for an array with shape (48000, 3072) and data type bool\n",
      "\n",
      "\n",
      "error 4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 247, in cross_validation\n",
      "    train_predictions, train_probs = curr_model.predict_wrapper(X_train)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/basemodel.py\", line 104, in predict_wrapper\n",
      "    self.predictions, self.prediction_probabilities = self.predict(X)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/basemodel.py\", line 144, in predict\n",
      "    self.prediction_probabilities = self.predict_proba(X)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/tree_models.py\", line 237, in predict_proba\n",
      "    probabilities = self.model.predict(X)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/lightgbm/basic.py\", line 3538, in predict\n",
      "    return predictor.predict(data, start_iteration, num_iteration,\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/lightgbm/basic.py\", line 848, in predict\n",
      "    preds, nrow = self.__pred_for_np2d(data, start_iteration, num_iteration, predict_type)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/lightgbm/basic.py\", line 938, in __pred_for_np2d\n",
      "    return inner_predict(mat, start_iteration, num_iteration, predict_type)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/lightgbm/basic.py\", line 898, in inner_predict\n",
      "    data = np.array(mat.reshape(mat.size), dtype=mat.dtype, copy=False)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.10 GiB for an array with shape (48000, 3072) and data type float64\n",
      "\n",
      "\n",
      "error 5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 217, in cross_validation\n",
      "    processed_data = process_data(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_data_processing.py\", line 41, in process_data\n",
      "    X_train = preprocessor.fit_transform(X_train)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 673, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 604, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 422, in fit_transform\n",
      "    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/base.py\", line 867, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 384, in fit\n",
      "    self.statistics_ = self._dense_fit(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 429, in _dense_fit\n",
      "    missing_mask = _get_mask(X, missing_values)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/_mask.py\", line 53, in _get_mask\n",
      "    return _get_dense_mask(X, value_to_mask)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/_mask.py\", line 20, in _get_dense_mask\n",
      "    Xt = np.isnan(X)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 141. MiB for an array with shape (48000, 3072) and data type bool\n",
      "\n",
      "\n",
      "error 6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 210, in cross_validation\n",
      "    raise TimeoutException(f\"time limit of {time_limit}s reached during fold {i}\")\n",
      "tabzilla_utils.TimeoutException: time limit of 7200s reached during fold 9\n",
      "\n",
      "\n",
      "error 7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 217, in cross_validation\n",
      "    processed_data = process_data(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_data_processing.py\", line 41, in process_data\n",
      "    X_train = preprocessor.fit_transform(X_train)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 697, in fit_transform\n",
      "    return self._hstack(list(Xs))\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 789, in _hstack\n",
      "    return np.hstack(Xs)\n",
      "  File \"<__array_function__ internals>\", line 5, in hstack\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/numpy/core/shape_base.py\", line 346, in hstack\n",
      "    return _nx.concatenate(arrs, 1)\n",
      "  File \"<__array_function__ internals>\", line 5, in concatenate\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 335. MiB for an array with shape (56000, 784) and data type float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(unique_errors):\n",
    "    print(f\"error {i}:\\n{e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some LightGBM errors are not due to time limit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----exception 0----\n",
      "occurrences: 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 247, in cross_validation\n",
      "    train_predictions, train_probs = curr_model.predict_wrapper(X_train)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/basemodel.py\", line 104, in predict_wrapper\n",
      "    self.predictions, self.prediction_probabilities = self.predict(X)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/basemodel.py\", line 144, in predict\n",
      "    self.prediction_probabilities = self.predict_proba(X)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/models/tree_models.py\", line 237, in predict_proba\n",
      "    probabilities = self.model.predict(X)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/lightgbm/basic.py\", line 3538, in predict\n",
      "    return predictor.predict(data, start_iteration, num_iteration,\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/lightgbm/basic.py\", line 848, in predict\n",
      "    preds, nrow = self.__pred_for_np2d(data, start_iteration, num_iteration, predict_type)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/lightgbm/basic.py\", line 938, in __pred_for_np2d\n",
      "    return inner_predict(mat, start_iteration, num_iteration, predict_type)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/lightgbm/basic.py\", line 898, in inner_predict\n",
      "    data = np.array(mat.reshape(mat.size), dtype=mat.dtype, copy=False)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.10 GiB for an array with shape (48000, 3072) and data type float64\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__CIFAR_10__167124\n",
      "--------\n",
      "----exception 1----\n",
      "occurrences: 27\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 217, in cross_validation\n",
      "    processed_data = process_data(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_data_processing.py\", line 41, in process_data\n",
      "    X_train = preprocessor.fit_transform(X_train)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 673, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 604, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 422, in fit_transform\n",
      "    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/base.py\", line 867, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 384, in fit\n",
      "    self.statistics_ = self._dense_fit(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 436, in _dense_fit\n",
      "    mean_masked = np.ma.mean(masked_X, axis=0)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/numpy/ma/core.py\", line 6774, in __call__\n",
      "    return method(marr, *args, **params)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/numpy/ma/core.py\", line 5244, in mean\n",
      "    cnt = self.count(axis=axis, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/numpy/ma/core.py\", line 4534, in count\n",
      "    return (~m).sum(axis=axis, dtype=np.intp, **kwargs)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 141. MiB for an array with shape (48000, 3072) and data type bool\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__CIFAR_10__167124\n",
      "--------\n",
      "----exception 2----\n",
      "occurrences: 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 217, in cross_validation\n",
      "    processed_data = process_data(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_data_processing.py\", line 41, in process_data\n",
      "    X_train = preprocessor.fit_transform(X_train)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 673, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 604, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 422, in fit_transform\n",
      "    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/base.py\", line 867, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 384, in fit\n",
      "    self.statistics_ = self._dense_fit(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 429, in _dense_fit\n",
      "    missing_mask = _get_mask(X, missing_values)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/_mask.py\", line 53, in _get_mask\n",
      "    return _get_dense_mask(X, value_to_mask)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/_mask.py\", line 20, in _get_dense_mask\n",
      "    Xt = np.isnan(X)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 141. MiB for an array with shape (48000, 3072) and data type bool\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__CIFAR_10__167124\n",
      "--------\n",
      "----exception 3----\n",
      "occurrences: 2\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 822, in dispatch_one_batch\n",
      "    tasks = self._ready_batches.get(block=False)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/queue.py\", line 168, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 217, in cross_validation\n",
      "    processed_data = process_data(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_data_processing.py\", line 41, in process_data\n",
      "    X_train = preprocessor.fit_transform(X_train)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 673, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 604, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 833, in dispatch_one_batch\n",
      "    islice = list(itertools.islice(iterator, big_batch_size))\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 607, in <genexpr>\n",
      "    X=_safe_indexing(X, column, axis=1),\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 361, in _safe_indexing\n",
      "    return _array_indexing(X, indices, indices_dtype, axis=axis)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/__init__.py\", line 185, in _array_indexing\n",
      "    return array[key] if axis == 0 else array[:, key]\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 524. MiB for an array with shape (4296, 16000) and data type float64\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__guillermo__168337\n",
      "openml__riccardo__168338\n",
      "--------\n",
      "----exception 4----\n",
      "occurrences: 56\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 217, in cross_validation\n",
      "    processed_data = process_data(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_data_processing.py\", line 41, in process_data\n",
      "    X_train = preprocessor.fit_transform(X_train)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 673, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 604, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 870, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/pipeline.py\", line 422, in fit_transform\n",
      "    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/base.py\", line 867, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 345, in fit\n",
      "    X = self._validate_input(X, in_fit=True)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/impute/_base.py\", line 287, in _validate_input\n",
      "    X = self._validate_data(\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/base.py\", line 577, in _validate_data\n",
      "    X = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 925, in check_array\n",
      "    array = np.array(array, dtype=dtype, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 524. MiB for an array with shape (16000, 4296) and data type float64\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__guillermo__168337\n",
      "openml__riccardo__168338\n",
      "--------\n",
      "----exception 5----\n",
      "occurrences: 2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_experiment.py\", line 136, in __call__\n",
      "    result = cross_validation(model, self.dataset, self.time_limit)\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_utils.py\", line 217, in cross_validation\n",
      "    processed_data = process_data(\n",
      "  File \"/home/shared/tabzilla/TabSurvey/tabzilla_data_processing.py\", line 41, in process_data\n",
      "    X_train = preprocessor.fit_transform(X_train)\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 697, in fit_transform\n",
      "    return self._hstack(list(Xs))\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 789, in _hstack\n",
      "    return np.hstack(Xs)\n",
      "  File \"<__array_function__ internals>\", line 5, in hstack\n",
      "  File \"/opt/conda/envs/gbdt/lib/python3.9/site-packages/numpy/core/shape_base.py\", line 346, in hstack\n",
      "    return _nx.concatenate(arrs, 1)\n",
      "  File \"<__array_function__ internals>\", line 5, in concatenate\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 335. MiB for an array with shape (56000, 784) and data type float64\n",
      "\n",
      "\n",
      "this exception occurs on the following datasets:\n",
      "openml__mnist_784__3573\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "non_timeout_errors = errors_df.loc[(errors_df[\"alg_name\"] == alg_name) & (~errors_df[\"exception\"].str.contains(\"Timeout\")) , \"exception\"].unique() #   \"dataset_name\", \"hparam_source\",\n",
    "\n",
    "for i, e in enumerate(non_timeout_errors):\n",
    "    print(f\"----exception {i}----\")\n",
    "    print(f\"occurrences: {len(errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)])}\")\n",
    "    print(e + \"\\n\")\n",
    "\n",
    "    er_datasets = errors_df.loc[(errors_df['alg_name'] == alg_name) & (errors_df['exception']==e)][\"dataset_name\"].unique()\n",
    "    print(\"this exception occurs on the following datasets:\")\n",
    "    for d in er_datasets:\n",
    "        print(d)\n",
    "\n",
    "    print(f\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors by Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
